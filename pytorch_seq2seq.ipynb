{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_seq2seq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "14rDmB4mwLP9ONA1iLd-puDLsTW1hWs_x",
      "authorship_tag": "ABX9TyPXn+y2ehR6B/cABNnGcKog",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanakakeitaro/r_d/blob/master/pytorch_seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhsE78yOB0pI",
        "colab_type": "code",
        "outputId": "90e98eb9-92eb-4da8-fa69-6c954dec1cd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSAXIDSx0q-M",
        "colab_type": "code",
        "outputId": "7b57c3a8-b6fe-4fc2-db01-b30d88928c84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd drive/My\\ Drive/marusen575"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/103/marusen575\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGiBWP3b3q6C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "def pickle_load(path):\n",
        "    with open(path, mode='rb') as f:\n",
        "        data = pickle.load(f)\n",
        "        return data\n",
        "kami5_data = pickle_load('marusen_kami5_list.pickle')\n",
        "naka7_data = pickle_load('marusen_naka7_list.pickle')\n",
        "\n",
        "id_to_word = pickle_load('id_to_word_marusen575.pickle')\n",
        "word_to_id = pickle_load('word_to_id_marusen575.pickle')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2t3gBScvRpo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 長さを5に合わせるため、2を末尾に加えてパディング\n",
        "index = 0\n",
        "for w in kami5_data:\n",
        "    while len(w) < 5:\n",
        "        w.append(2)\n",
        "    kami5_data[index] = w\n",
        "    index += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0t4JRwxx4bpG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 先頭に<bos>, 末尾に<eos>を加える\n",
        "[l.insert(0, 0) for l in naka7_data]\n",
        "[l.append(1) for l in naka7_data]\n",
        "\n",
        "# 長さを9に合わせるため、2を末尾に加えてパディング\n",
        "index = 0\n",
        "for w in naka7_data:\n",
        "    while len(w) < 9:\n",
        "        w.append(2)\n",
        "    naka7_data[index] = w\n",
        "    index += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3I3dtTGXAlNj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# train : test = 8 : 2 にデータをわける\n",
        "train_5, test_5, train_7, test_7 = train_test_split(kami5_data, naka7_data, test_size= 0.2, random_state=0)\n",
        "\n",
        "# データをバッチ化するための関数\n",
        "def train2batch(input_data, output_data, batch_size=100):\n",
        "    input_batch = []\n",
        "    output_batch = []\n",
        "    input_shuffle, output_shuffle = shuffle(input_data, output_data)\n",
        "    for i in range(0, len(input_data), batch_size):\n",
        "      input_batch.append(input_shuffle[i:i+batch_size])\n",
        "      output_batch.append(output_shuffle[i:i+batch_size])\n",
        "    return input_batch, output_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v394Q_QKm1BO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "embedding_dim = 50\n",
        "hidden_dim = 128\n",
        "vocab_size = len(word_to_id)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eViHQqzXHK2e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, batch_size=100):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim, )\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, indices):\n",
        "        embedding = self.word_embeddings(indices)\n",
        "        if embedding.dim() == 2:\n",
        "            embedding = torch.unsqueeze(embedding, 1)\n",
        "        _, state = self.gru(embedding, torch.zeros(1, self.batch_size, self.hidden_dim, device=device))\n",
        "        \n",
        "        return state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWxGzED_bYgD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, batch_size=100):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim, )\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.output = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, index, state):\n",
        "        embedding = self.word_embeddings(index)\n",
        "        if embedding.dim() == 2:\n",
        "            embedding = torch.unsqueeze(embedding, 1)\n",
        "        gruout, state = self.gru(embedding, state)\n",
        "        output = self.output(gruout)\n",
        "        return output, state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXt4XpKEl1WD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GPU使えるように\n",
        "encoder = Encoder(vocab_size, embedding_dim, hidden_dim).to(device)\n",
        "decoder = Decoder(vocab_size, embedding_dim, hidden_dim).to(device)\n",
        "\n",
        "# 損失関数\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 最適化\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07qq8nDumWoc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import datetime\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "batch_size = 100\n",
        "def train2batch(data, target, batch_size=100):\n",
        "    input_batch = []\n",
        "    output_batch = []\n",
        "    input_shuffle, output_shuffle = shuffle(data, target)\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        input_batch.append(input_shuffle[i:i+batch_size])\n",
        "        output_batch.append(output_shuffle[i:i+batch_size])\n",
        "    input_batch.pop(-1)\n",
        "    return input_batch, output_batch\n",
        "\n",
        "def get_current_time():\n",
        "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08U2AsD93gMJ",
        "colab_type": "code",
        "outputId": "2ea95281-bba1-432c-dea9-935251073087",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "print(\"Training…\")\n",
        "n_epoch = 100\n",
        "for epoch in range(1, n_epoch+1):\n",
        "    input_batch, output_batch = train2batch(train_5, train_7)\n",
        "    for i in range(len(input_batch)):\n",
        "        # 勾配の初期化\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "        # データをテンソルに変換\n",
        "        inputs = torch.tensor(input_batch[i], device=device)\n",
        "        outputs = torch.tensor(output_batch[i], device=device)\n",
        "        # Encoderの順伝播\n",
        "        encoder_hidden = encoder(inputs)\n",
        "        # Decoderで使うデータはoutput_tensorを１つずらしたものを使う\n",
        "        # Decoderのインプットとするデータ\n",
        "        source = outputs[:, :-1]\n",
        "        # Decoderの教師データ\n",
        "        # 生成開始を表す\"<bos>\"を削っている\n",
        "        target = outputs[:, 1:]\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        # Forward batch of sequences through decoder one time step at a time\n",
        "        loss = 0\n",
        "        for i in range(source.size(1)):\n",
        "            decoder_output, decoder_hidden = decoder(source[:, i], decoder_hidden)\n",
        "            decoder_output = torch.squeeze(decoder_output)\n",
        "            loss += criterion(decoder_output, target[:, i])\n",
        "        \n",
        "        # 誤差逆伝播\n",
        "        loss.backward()\n",
        "\n",
        "        # パラメータ更新\n",
        "        # Encoder、Decoder両方学習\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(get_current_time(), \"Epoch %d: %.2f\" % (epoch, loss.item()))    \n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        model_name = \"seq2seq_calculator_v{}.pt\".format(epoch)\n",
        "        torch.save({\n",
        "            'encoder_model': encoder.state_dict(),\n",
        "            'decoder_model': decoder.state_dict(),\n",
        "        }, model_name)\n",
        "        print(\"Saving the checkpoint...\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training…\n",
            "2020-05-18 14:24:04 Epoch 10: 17.67\n",
            "Saving the checkpoint...\n",
            "2020-05-18 14:36:34 Epoch 20: 12.52\n",
            "Saving the checkpoint...\n",
            "2020-05-18 14:49:04 Epoch 30: 7.30\n",
            "Saving the checkpoint...\n",
            "2020-05-18 15:01:29 Epoch 40: 4.73\n",
            "Saving the checkpoint...\n",
            "2020-05-18 15:13:58 Epoch 50: 3.18\n",
            "Saving the checkpoint...\n",
            "2020-05-18 15:26:25 Epoch 60: 2.32\n",
            "Saving the checkpoint...\n",
            "2020-05-18 15:38:51 Epoch 70: 1.69\n",
            "Saving the checkpoint...\n",
            "2020-05-18 15:51:12 Epoch 80: 1.31\n",
            "Saving the checkpoint...\n",
            "2020-05-18 16:03:35 Epoch 90: 1.14\n",
            "Saving the checkpoint...\n",
            "2020-05-18 16:15:54 Epoch 100: 1.26\n",
            "Saving the checkpoint...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDIBbfLhrtKs",
        "colab_type": "code",
        "outputId": "452cf1a0-6660-4a62-8d7e-aee00357aa44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "encoder = Encoder(vocab_size, embedding_dim, hidden_dim, batch_size=1).to(device)\n",
        "decoder = Decoder(vocab_size, embedding_dim, hidden_dim, batch_size=1).to(device)\n",
        "\n",
        "def indices2pre(argsorted, state):\n",
        "    predict_7 = [argsorted]\n",
        "    for _ in range(8):\n",
        "        input_tensor = torch.tensor([argsorted], device=device)\n",
        "        output, state = decoder(input_tensor, state)\n",
        "        # 配列の最大値のインデックスを返す\n",
        "        prob = F.softmax(torch.squeeze(output))\n",
        "        argsorted = torch.argmax(prob.cpu().detach()).item()\n",
        "        predict_7.append(argsorted)\n",
        "    return predict_7\n",
        "\n",
        "for epoch in range(10, 101, 10):\n",
        "    model_name = \"seq2seq_calculator_v{}.pt\".format(epoch)\n",
        "    checkpoint = torch.load(model_name)\n",
        "    encoder.load_state_dict(checkpoint[\"encoder_model\"])\n",
        "    decoder.load_state_dict(checkpoint[\"decoder_model\"])\n",
        "\n",
        "    print(\"Checkpoint {:>3d}\".format(epoch))\n",
        "    print(\"-\"*30)\n",
        "    accuracy = 0\n",
        "    i = 46\n",
        "    input_5 = [test_5[i]]\n",
        "    answer_7 = [test_7[i]]\n",
        "    with torch.no_grad():\n",
        "        for x_5, y_7 in zip(input_5, answer_7):\n",
        "            pri5 = [id_to_word[i] for i in x_5 if i != 2]\n",
        "            print(pri5)\n",
        "            # テンソルに変換\n",
        "            input_tensor = torch.tensor([x_5], device=device)\n",
        "            # encoderは隠れ状態を返す\n",
        "            state = encoder(input_tensor)\n",
        "            # 変数tokenいらないけどわかりやすさのために\n",
        "            token = '<bos>'\n",
        "            predict_7 = [word_to_id[token]]\n",
        "            # 推論\n",
        "            index = word_to_id[token]\n",
        "            input_tensor = torch.tensor([index], device=device)\n",
        "            output, state = decoder(input_tensor, state)\n",
        "            # outputをsoftmaxで確率に変換し、大きい順に並べ替える\n",
        "            prob = F.softmax(torch.squeeze(output))\n",
        "            indices = torch.argsort(prob.cpu().detach(), descending=True)\n",
        "            # 並び替えたリストをもとに、値が大きい順に予測していく\n",
        "            for i in indices[:3]:\n",
        "                i = i.item()\n",
        "                pre_7 = indices2pre(i, state)\n",
        "                predict = [id_to_word[i] for i in pre_7 if i != 2]\n",
        "                print(predict)\n",
        "            index = torch.argmax(prob.cpu().detach()).item()\n",
        "            print(index)\n",
        "            \"\"\"\n",
        "            # accuracyを足していく\n",
        "            answer = [id_to_word[i] for i in y_7 if i != 2]\n",
        "            predict = [id_to_word[i] for i in predict_7 if i != 2]\n",
        "            print(answer, predict)\n",
        "            flag = [\"F\", \"T\"][answer == predict]\n",
        "            if flag == \"T\":\n",
        "                accuracy += 1\n",
        "    print(\"Accuracy: {:.5f}\".format(accuracy / len(input_5)))\n",
        "    print(\"-\"*30)\n",
        "    \"\"\""
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checkpoint  10\n",
            "------------------------------\n",
            "['浮気', 'する']\n",
            "['君', 'の', '中', 'に', '<eos>']\n",
            "['人', 'の', 'よう', 'に', '<eos>']\n",
            "['妻', 'の', 'よう', 'に', '<eos>']\n",
            "344\n",
            "Checkpoint  20\n",
            "------------------------------\n",
            "['浮気', 'する']\n",
            "['君', 'の', '嗚咽', 'に', '<eos>']\n",
            "['時', 'は', '私', 'の', '<eos>']\n",
            "['俺', 'の', '小言', 'は', '<eos>']\n",
            "344\n",
            "Checkpoint  30\n",
            "------------------------------\n",
            "['浮気', 'する']\n",
            "['財布', 'の', 'そば', 'に', '<eos>']\n",
            "['君', 'の', '体温', '<eos>']\n",
            "['時', 'は', '必ず', '<eos>']\n",
            "598\n",
            "Checkpoint  40\n",
            "------------------------------\n",
            "['浮気', 'する']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:45: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['財布', 'は', '無い', 'の', '<eos>']\n",
            "['犬', 'の', 'ホテル', 'が', '<eos>']\n",
            "['その', '為', 'は', 'し', 'ない', '<eos>']\n",
            "598\n",
            "Checkpoint  50\n",
            "------------------------------\n",
            "['浮気', 'する']\n",
            "['財布', 'は', '無い', 'の', '<eos>']\n",
            "['犬', 'も', '寒く', 'て', '<eos>']\n",
            "['その', '為', 'は', 'し', 'て', '<eos>']\n",
            "598\n",
            "Checkpoint  60\n",
            "------------------------------\n",
            "['浮気', 'する']\n",
            "['財布', 'は', '無い', 'が', '<eos>']\n",
            "['時', 'は', '必ず', '<eos>']\n",
            "['口', 'は', 'パパ', 'が', '<eos>']\n",
            "598\n",
            "Checkpoint  70\n",
            "------------------------------\n",
            "['浮気', 'する']\n",
            "['財布', 'は', '知ら', 'ない', '<eos>']\n",
            "['口', 'の', '方', 'が', '<eos>']\n",
            "['時', 'は', '必ず', '<eos>']\n",
            "598\n",
            "Checkpoint  80\n",
            "------------------------------\n",
            "['浮気', 'する']\n",
            "['財布', 'は', '知ら', 'ない', '<eos>']\n",
            "['娘', 'と', '悩み', 'を', '<eos>']\n",
            "['彼女', 'の', '隣', 'で', '<eos>']\n",
            "598\n",
            "Checkpoint  90\n",
            "------------------------------\n",
            "['浮気', 'する']\n",
            "['財布', 'は', '知ら', 'ない', '<eos>']\n",
            "['時', 'は', '必ず', '<eos>']\n",
            "['君', '待つ', '日', 'だけ', 'は', '<eos>']\n",
            "598\n",
            "Checkpoint 100\n",
            "------------------------------\n",
            "['浮気', 'する']\n",
            "['財布', 'は', '知ら', 'ない', '<eos>']\n",
            "['口', 'の', '膝', 'に', '<eos>']\n",
            "['笑顔', 'て', 'い', 'ない', '<eos>']\n",
            "598\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wxhIpKxijoj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1id0r9Bkj0au",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w, e, r = a[:3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wsdk3WlwkJOf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2a512081-7dc1-4611-c93b-1b315e60f07f"
      },
      "source": [
        "type(w)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5IIUggXkXB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}